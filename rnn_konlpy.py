# -*- coding: utf-8 -*-
"""RNN_Konlpy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IvZ1V6sbjnrlY9BkCqbvROnOqSq_JnUE

# **한국어 자연어 처리**

환경설정

https://colab.research.google.com/drive/1tL2WjfE0v_es4YJCLGoEJM5NXs_O_ytW#scrollTo=1lxZgy_vjaah //출처
"""

!pip install konlpy

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!pip3 install JPype1-py3

import os
os.chdir('/tmp/')
!curl -LO https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.1.tar.gz
!tar zxfv mecab-0.996-ko-0.9.1.tar.gz
os.chdir('/tmp/mecab-0.996-ko-0.9.1')
!./configure
!make
!make check
!make install

os.chdir('/tmp')
!curl -LO http://ftpmirror.gnu.org/automake/automake-1.11.tar.gz
!tar -zxvf automake-1.11.tar.gz
os.chdir('/tmp/automake-1.11')
!./configure
!make
!make install

import os
os.chdir('/tmp/')

 
!wget -O m4-1.4.9.tar.gz http://ftp.gnu.org/gnu/m4/m4-1.4.9.tar.gz
!tar -zvxf m4-1.4.9.tar.gz
os.chdir('/tmp/m4-1.4.9')
!./configure
!make
!make install

os.chdir('/tmp')
!curl -OL http://ftpmirror.gnu.org/autoconf/autoconf-2.69.tar.gz
!tar xzf autoconf-2.69.tar.gz
os.chdir('/tmp/autoconf-2.69')
!./configure --prefix=/usr/local
!make
!make install
!export PATH=/usr/local/bin

os.chdir('/tmp')
!curl -LO http://ftpmirror.gnu.org/automake/automake-1.11.tar.gz
!tar -zxvf automake-1.11.tar.gz
os.chdir('/tmp/automake-1.11')
!./configure
!make
!make install

import os
os.chdir('/tmp')
!curl -LO https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.0.1-20150920.tar.gz
!tar -zxvf mecab-ko-dic-2.0.1-20150920.tar.gz
os.chdir('/tmp/mecab-ko-dic-2.0.1-20150920')
!./autogen.sh
!./configure
!make
# !sh -c 'echo "dicdir=/usr/local/lib/mecab/dic/mecab-ko-dic" > /usr/local/etc/mecabrc'
!make install

os.chdir('/tmp/mecab-ko-dic-2.0.1-20150920')
!ldconfig
!ldconfig -p | grep /usr/local/lib

import os
os.chdir('/tmp')
!curl -LO https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.0.1-20150920.tar.gz
!tar -zxvf mecab-ko-dic-2.0.1-20150920.tar.gz
os.chdir('/tmp/mecab-ko-dic-2.0.1-20150920')
!./autogen.sh
!./configure
!make
# !sh -c 'echo "dicdir=/usr/local/lib/mecab/dic/mecab-ko-dic" > /usr/local/etc/mecabrc'
!make install

# install mecab-python
import os
os.chdir('/content')

!git clone https://bitbucket.org/eunjeon/mecab-python-0.996.git
os.chdir('/content/mecab-python-0.996')

!python3 setup.py build
!python3 setup.py install

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib as mat
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import re
import json

# %matplotlib inline

from konlpy.tag import Mecab

from google.colab import auth
auth.authenticate_user()

from google.colab import drive
drive.mount('/content/gdrive')

from tensorflow.python.keras.preprocessing.sequence import pad_sequences
from tensorflow.python.keras.preprocessing.text import Tokenizer

mecab = Mecab()

DATA_PATH = "/content/gdrive/My Drive/First_data/Data1.xlsx"

train_data = pd.read_excel(DATA_PATH)

train_data.head()

print('전체 데이터의 개수 : {}'.format(len(train_data)))

train_length = train_data['Text'].apply(len)
train_length.head()
# 문자열의 길이 파악

plt.figure(figsize=(12,5))
plt.hist(train_length,bins=200,alpha=0.5,color='r',label='word')
plt.title('Histogram of length of review')
plt.xlabel('Length of review')
plt.ylabel('Number of review')
# 문자열의 길이를 히스토그램으로 시각화

print('리뷰 길이 최댓값 : {}'.format(np.max(train_length)))
print('리뷰 길이 최솟값 : {}'.format(np.min(train_length)))
print('리뷰 길이 평균값 : {}'.format(np.mean(train_length)))
print('리뷰 길이 표준편차 : {}'.format(np.std(train_length)))
print('리뷰 길이 중간값 : {}'.format(np.median(train_length)))
# 문자열 데이터 분포 파악

plt.figure(figsize=(12,5))
plt.boxplot(train_length,labels=['counts'],showmeans=True)

train_data['Value'].value_counts().plot(kind='bar')
# 1 : 질문 & 0: 질문이 아닌 Text 의 갯수 파악

"""데이터를 Train_data, Text_Data 로 나누기"""

X_train = train_data[:370]
X_test = train_data[370:520]

X_train.head()

X_test.head()

"""데이터 전처리"""

def preprocessing(text, mecab, remove_stopwords = False, stop_words = []):
  re_text = re.sub("[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]","",text);
  word_text = mecab.morphs(re_text)

  if remove_stopwords:
    word_text = [token for token in word_text if not token in stop_words]

  return word_text

stop_words = ['은','는','이','가','로','하','아','들','것','의','있','되','수','보','주','등']

X_train_word_text = []

for text in X_train['Text']:
  if type(text) == str :
    X_train_word_text.append(preprocessing(text,mecab,remove_stopwords=True,stop_words=stop_words))
  else:
    X_train_word_text.append([])

X_train_word_text[:4]

X_test_word_text = []

for text in X_test['Text']:
  if type(text) == str :
    X_test_word_text.append(preprocessing(text,mecab,remove_stopwords=True,stop_words=stop_words))
  else:
    X_test_word_text.append([])

X_test_word_text[:4]

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train_word_text)

word_to_index = tokenizer.word_index
print(word_to_index)

vocab_size = len(word_to_index) + 1
print('단어 집합의 크기 : {}'.format(vocab_size))

threshold = 2
total_cnt = len(word_to_index) #단어의 수

rare_cnt = 0 #등장빈도수가 threshold 보다 작은 단어의 개수
total_freq = 0 #훈련 데이터의 전체 단어 빈도수 총 합
rare_freq = 0 #등장 빈도수가 threshold 보다 작은 단어의 등장 빈도수의 총 합

for key, value in tokenizer.word_counts.items():
  total_freq = total_freq + value

  if(value < threshold):
    rare_cnt = rare_cnt +1
    rare_freq = rare_freq + value

print('등장 빈도가 %s 번 이하인 희귀 단어의 수 : %s'%(threshold - 1, rare_cnt))
print("단어 집합에서 희귀 단어의 비율 : ", (rare_cnt/total_cnt)*100)
print("전체 등장 빈도에서 희귀 단어의 등장 빈도 비율 : ", (rare_freq/total_freq)*100)

"""RNN을 사용해서 Text 분류 준비하기"""

X_train_for = tokenizer.texts_to_sequences(X_train_word_text)
X_test_for = tokenizer.texts_to_sequences(X_test_word_text)

print(X_train_for[:3])

Y_train = np.array(X_train['Value'])
Y_test = np.array(X_test['Value'])

print(Y_train[:3])

print(len(X_train_for))
print(len(Y_train))

print('Vector의 최대 길이 :',max(len(l) for l in X_train_for))
print('Vecotr의 평균 길이 :',sum(map(len, X_train))/len(X_train_for))
plt.hist([len(s) for s in X_train_for], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

def below_threshold_len(max_len, nested_list):
  cnt = 0
  for s in nested_list:
    if(len(s) <= max_len):
        cnt = cnt + 1
  print('전체 Vector 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))

max_len = 21
below_threshold_len(max_len, X_train_for)

X_train_last = pad_sequences(X_train_for, maxlen = max_len)
#모든 데이터의 길이를 20으로 맞추기 위해서 패딩작업

from tensorflow.keras.layers import Embedding, Dense, LSTM
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

model = Sequential()
model.add(Embedding(vocab_size, 100))
model.add(LSTM(128))
model.add(Dense(1, activation='sigmoid'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
history = model.fit(X_train_last, Y_train, epochs=15, callbacks=[es, mc], batch_size=30, validation_split=0.2)

model.summary()

X_test_last = pad_sequences(X_test_for, maxlen = max_len)

print("\n 테스트 정확도: %.4f" % (model.evaluate(X_test_last, Y_test)[1]))

plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

